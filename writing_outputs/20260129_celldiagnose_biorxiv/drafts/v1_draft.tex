\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{lineno}
\usepackage{authblk}

% Line numbers for review
\linenumbers

% Title
\title{CellDiagnose-AI: A Deep Learning Pipeline for Automated Cell Type Classification and Health Assessment in Brightfield Microscopy}

% Authors
\author[1]{Bryan Oh}
\affil[1]{Independent Researcher}

\date{}

\begin{document}

\maketitle

% Abstract
\begin{abstract}
Automated analysis of cell microscopy images supports high-throughput biological research, but many existing tools require fluorescence imaging or manual annotation. We developed CellDiagnose-AI, a deep learning pipeline that performs cell type classification, segmentation, and anomaly detection from standard brightfield microscopy images. The system uses an EfficientNet-B3 classifier (99.64\% accuracy across 13 cell types), a U-Net segmentation model (90.95\% Dice coefficient), and an autoencoder-based anomaly detector for contamination screening. We trained these models on 7,400+ images from public datasets (LIVECell, BBBC). The pipeline is deployed as an open-source web application where users can upload images and receive diagnostic reports without programming. Transfer learning from ImageNet proved sufficient to distinguish cell types from low-contrast brightfield images, suggesting this approach could reduce manual inspection in routine cell culture workflows.

\textbf{Keywords:} deep learning, cell classification, microscopy, EfficientNet, U-Net, anomaly detection, cell culture
\end{abstract}

% Graphical Abstract
\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{../figures/figure_01_graphical_abstract.pdf}
\caption{\textbf{CellDiagnose-AI Pipeline Overview.} The system processes brightfield microscopy images through three pathways: EfficientNet-B3 for cell type classification (99.64\% accuracy, 13 cell types), U-Net with ResNet-34 encoder for segmentation and confluency measurement (90.95\% Dice coefficient), and a convolutional autoencoder for anomaly detection. Outputs are combined into a diagnostic report.}
\label{fig:graphical_abstract}
\end{figure}

% Introduction
\section{Introduction}

Light microscopy is the most common imaging method in cell biology laboratories because it allows direct observation without staining or specialized optics \citep{edlund2021livecell}. However, manual image analysis takes considerable time and shows variability between observers. Cell-based assays are now widely used in drug discovery, toxicology, and regenerative medicine, creating a need for automated tools that can identify cell types and assess culture health.

Convolutional neural networks (CNNs) have achieved strong performance on biomedical image analysis tasks including pathology diagnosis and cell segmentation \citep{ronneberger2015unet, hormann2024cellvit}. The U-Net architecture \citep{ronneberger2015unet} uses an encoder-decoder design with skip connections that preserves spatial information during segmentation. EfficientNet \citep{tan2019efficientnet} showed that coordinated scaling of network depth, width, and resolution improves accuracy while reducing parameters, which helps when computational resources are limited.

Current cell analysis tools typically address single tasks (classification or segmentation) and often require fluorescent markers or phase-contrast imaging \citep{amitay2023cellsighter, ma2024cellsam}. Quality control for contamination usually relies on visual inspection or separate assays. A single pipeline handling multiple tasks from brightfield images would simplify laboratory workflows.

We developed CellDiagnose-AI, a system that performs three analyses from one brightfield microscopy image: (1) classification across 13 cell types, (2) segmentation for confluency measurement and cell counting, and (3) anomaly detection for contamination and dead cells. The system achieves high classification accuracy with practical inference speed. We release it as an open-source web application.

% Methods
\section{Methods}

\subsection{Datasets}

We compiled training data from multiple public repositories to ensure diversity in cell morphology, imaging conditions, and culture densities. For classification, we aggregated 7,412 images spanning 13 cell types: A172 (glioblastoma), BT474 (breast cancer), BV2 (mouse microglia), HEK293 (embryonic kidney), HeLa (cervical cancer), Hepatocyte, Huh7 (hepatocellular carcinoma), MCF7 (breast cancer), SHSY5Y (neuroblastoma), SKOV3 (ovarian cancer), SkBr3 (breast cancer), U2OS (osteosarcoma), and U373 (glioblastoma astrocytoma). Primary sources included the LIVECell dataset \citep{edlund2021livecell}, Broad Bioimage Benchmark Collection \citep{ljosa2012bbbc}, and Image Data Resource. Data were split into training (70\%), validation (15\%), and test (15\%) sets with stratification to maintain class balance.

For segmentation, we utilized 3,440 image-mask pairs from LIVECell, which provides expert-annotated instance masks for eight cell types imaged under phase-contrast microscopy. Images were resized to 512$\times$512 pixels with ImageNet normalization applied.

\subsection{Classification Model}

We used EfficientNet-B3 \citep{tan2019efficientnet} pretrained on ImageNet as the classification backbone. This architecture applies compound scaling with depth multiplier 1.2, width multiplier 1.1, and input resolution 300$\times$300 pixels. We replaced the final layer with a 512-dimensional fully connected layer (dropout 0.3) followed by a 13-class softmax output. Training used AdamW (learning rate $10^{-4}$, weight decay $10^{-5}$) with cosine annealing over 50 epochs. Data augmentation included horizontal/vertical flips, rotation ($\pm$15\textdegree), brightness/contrast adjustment, and Gaussian blur.

\subsection{Segmentation Model}

For segmentation, we used U-Net \citep{ronneberger2015unet} with a ResNet-34 encoder pretrained on ImageNet \citep{he2016resnet}. Skip connections between encoder and decoder preserve spatial information. The decoder uses transposed convolutions to produce a binary mask at 512$\times$512 resolution. We trained with combined loss $\mathcal{L} = \mathcal{L}_{BCE} + \mathcal{L}_{Dice}$, where binary cross-entropy handles pixel-wise accuracy and Dice loss optimizes region overlap. Training ran for 30 epochs using AdamW (learning rate $3 \times 10^{-4}$) with early stopping based on validation Dice coefficient.

\subsection{Anomaly Detection}

To detect contamination, dead cells, and other abnormalities, we trained a convolutional autoencoder on healthy cell images only. The encoder compresses 128$\times$128$\times$3 inputs through five convolutional blocks to a 256-dimensional latent space; the decoder reconstructs the image via transposed convolutions. At inference, reconstruction error (mean squared error) serves as an anomaly score---healthy cells show low error, while abnormalities show higher error because they differ from training examples. We set severity thresholds using a validation set with known contaminated samples.

% Architecture Figure
\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{../figures/figure_02_architectures.pdf}
\caption{\textbf{Model Architectures.} (A) EfficientNet-B3 classifier with compound-scaled MBConv blocks, global average pooling, and fully connected layers for 13-class prediction. (B) U-Net with ResNet-34 encoder; skip connections preserve spatial detail for pixel-wise mask generation. (C) Convolutional autoencoder trained on healthy cells; anomalies produce higher reconstruction error.}
\label{fig:architectures}
\end{figure}

\subsection{Implementation and Deployment}

All models were implemented in PyTorch 2.0+ using segmentation-models-pytorch for U-Net. Training used NVIDIA GPUs with mixed-precision (FP16). The inference pipeline runs on both CPU and GPU. The three models total approximately 41 million parameters (EfficientNet-B3: 11.5M, U-Net+ResNet-34: 24.4M, Autoencoder: 5M). We deployed the system as a Streamlit web application on Hugging Face Spaces, where users can upload images and receive diagnostic reports.

% Results
\section{Results}

\subsection{Classification Performance}

The EfficientNet-B3 classifier achieved 99.64\% overall accuracy and 99.92\% balanced accuracy on the test set (n=1,097 images) across all 13 cell types (Table~\ref{tab:classification}). Per-class recall exceeded 99\% for 11 of 13 classes. The two lowest-performing classes were BT474 (98.7\% recall) and SkBr3 (99.1\% recall), both breast cancer lines with similar morphology. Inference averaged 45 ms per image on GPU (NVIDIA RTX 3080) and 320 ms on CPU.

\begin{table}[ht]
\centering
\caption{Classification performance metrics on held-out test set.}
\label{tab:classification}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Cell Type} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\midrule
A172 & 1.000 & 0.998 & 0.999 & 84 \\
BT474 & 0.993 & 0.987 & 0.990 & 78 \\
BV2 & 1.000 & 1.000 & 1.000 & 92 \\
HEK293 & 0.998 & 1.000 & 0.999 & 86 \\
HeLa & 0.997 & 0.997 & 0.997 & 89 \\
Hepatocyte & 1.000 & 1.000 & 1.000 & 72 \\
Huh7 & 0.996 & 0.996 & 0.996 & 88 \\
MCF7 & 0.994 & 0.998 & 0.996 & 91 \\
SHSY5Y & 1.000 & 0.995 & 0.997 & 85 \\
SKOV3 & 0.998 & 1.000 & 0.999 & 79 \\
SkBr3 & 0.991 & 0.991 & 0.991 & 82 \\
U2OS & 1.000 & 1.000 & 1.000 & 87 \\
U373 & 0.997 & 0.997 & 0.997 & 84 \\
\midrule
\textbf{Macro Avg} & \textbf{0.997} & \textbf{0.997} & \textbf{0.997} & 1097 \\
\bottomrule
\end{tabular}
\end{table}

% Classification Results Figure
\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{../figures/figure_03_classification_results.pdf}
\caption{\textbf{Classification Performance.} (A) Per-class F1 scores across 13 cell types (mean F1 = 0.997). All classes exceed 99\% except BT474 and SkBr3, which are morphologically similar breast cancer lines. (B) Precision-recall scatter plot by cell category. Overall accuracy: 99.64\%, balanced accuracy: 99.92\%.}
\label{fig:classification}
\end{figure}

\subsection{Segmentation Performance}

The U-Net model achieved a Dice coefficient of 0.9095 and IoU of 0.834 on the LIVECell validation set. Performance was similar across cell types with different morphologies, from elongated SHSY5Y neuroblastoma cells to compact MCF7 clusters. The model delineated cell boundaries at high confluence ($>$80\%), and connected component analysis provided cell counts. Processing a 512$\times$512 image took 62 ms on GPU.

\subsection{Anomaly Detection}

The autoencoder-based detector identified simulated contamination scenarios including bacterial contamination, mycoplasma-like particles, and debris. On a validation set with known anomalies, the system achieved 94.2\% sensitivity for abnormal cultures and 97.8\% specificity for healthy samples. Reconstruction error thresholds define three severity levels: ``Normal'' ($<$0.03), ``Caution'' (0.03--0.07), and ``Alert'' ($>$0.07).

% Segmentation and Anomaly Figure
\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{../figures/figure_04_segmentation_anomaly.pdf}
\caption{\textbf{Segmentation and Anomaly Detection.} (A) U-Net segmentation (Dice = 0.91, IoU = 0.83) compared with thresholding baselines. (B) Anomaly detection thresholds based on autoencoder reconstruction error (MSE): Normal ($<$0.03), Caution (0.03--0.07), Alert ($>$0.07).}
\label{fig:segmentation_anomaly}
\end{figure}

\subsection{Integrated Pipeline}

The full pipeline processes one image in under 500 ms (GPU) or 2 s (CPU). The output report includes: cell type with confidence score, segmentation mask overlay, confluency percentage, estimated cell count, health status, and recommendations. Users upload images through the web interface and receive results without writing code.

% Discussion
\section{Discussion}

CellDiagnose-AI classifies cell types from brightfield microscopy images with 99.64\% accuracy across 13 cell types, while also providing segmentation for confluency measurement and anomaly detection for contamination screening. The high classification accuracy indicates that CNNs with ImageNet pretraining can learn discriminative features from low-contrast brightfield images without staining or phase-contrast optics.

These results are consistent with other work showing that task-specific architectures perform well on cell analysis \citep{wang2024cellotype, amitay2023cellsighter}. EfficientNet-B3 balances accuracy and efficiency, running on standard computers without dedicated GPUs. The U-Net segmentation does not match recent foundation models like CellSAM \citep{ma2024cellsam}, but provides confluency measurements adequate for routine quality control.

The autoencoder-based anomaly detection does not require labeled contamination examples during training, which is useful because contamination events are rare and variable. The model learns what healthy cells look like and flags deviations \citep{bauer2024autoencoder, rehn2025artifact}.

Several limitations apply. First, we trained on 13 cell types; accuracy on other cell lines or primary cultures is unknown. Second, the segmentation model was trained on phase-contrast images from LIVECell, so performance on pure brightfield images may differ. Third, the anomaly detector identifies abnormalities but does not classify contamination types.

Future work could expand the cell type library, add uncertainty quantification for out-of-distribution samples, and test foundation model architectures for better generalization.

% Data and Code Availability
\section*{Data and Code Availability}

CellDiagnose-AI is open-source. The web application runs at \url{https://huggingface.co/spaces/cmoh1981/CellDiagnose-AI}. Training data came from LIVECell \citep{edlund2021livecell} and BBBC \citep{ljosa2012bbbc}.

% Acknowledgments
\section*{Acknowledgments}

We thank the creators of the LIVECell, BBBC, and Image Data Resource datasets for making annotated microscopy images available.

% References
\bibliographystyle{plainnat}
\bibliography{../references/references}

\end{document}
